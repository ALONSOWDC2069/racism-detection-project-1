{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1V7qfmQXs8g",
    "outputId": "838e376e-3c8d-4145-e561-e4622b3d8e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (2.24.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from keras-tuner) (7.22.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (3.0.17)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (52.0.0.post20210125)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.17.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (2.8.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from ipython->keras-tuner) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.0.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from requests->keras-tuner) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from requests->keras-tuner) (1.25.10)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.31.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (0.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (2.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (1.21.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard->keras-tuner) (3.13.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\tru-pro\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\tru-pro\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "INsvANBUQ_Nn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2b705b9d0bdb>:38: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#importing libraries for model evaluation and algorithms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#dl libraraies\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization,Reshape,Dot,Concatenate,Add,Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import cv2\n",
    "from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# specifically for deeplearning.\n",
    "from tensorflow.keras.layers import Dropout, Flatten,Activation,Input,Embedding\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "DoU4sypRQ_Gj",
    "outputId": "f978d14b-7ba3-4d40-ce93-75637866a64e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                           sentence\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataframe\n",
    "df=pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SoDUJUsPU0M3"
   },
   "outputs": [],
   "source": [
    "#Setting parameters which will be used throughout\n",
    "num_words = 15000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "val_size = 1000  # Size of the validation set\n",
    "epochs = 20  # Number of epochs we usually start to train with\n",
    "batch_size = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "#Taking only two columns since it's a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jm0puHxWrnk",
    "outputId": "bbdded70-d311-4966-f6e9-0f10daba1fdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tru-\n",
      "[nltk_data]     pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tru-\n",
      "[nltk_data]     pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tru-\n",
      "[nltk_data]     pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tru-pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iJTh5-ShU0P2"
   },
   "outputs": [],
   "source": [
    "#tweets conssits of every document as an array of tokenized words which are later appended to docs \n",
    "tweets=[word_tokenize(tweet) for tweet in df['sentence']]\n",
    "docs=[]\n",
    "for j in range(0,len(tweets)):\n",
    "    docs.append(tweets[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F-rXkF_1U0iE"
   },
   "outputs": [],
   "source": [
    "#stops included both the stopwords and punctuations\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "not_list = [\"n't\", \"not\", \"no\"]\n",
    "stops.update(punctuations)\n",
    "stops.update(not_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1mxGxUnBWSv3"
   },
   "outputs": [],
   "source": [
    "#to get the simple pos(part of speech) tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_nO4zHxZWSzJ"
   },
   "outputs": [],
   "source": [
    "#to get the pos tag for a word\n",
    "from nltk import pos_tag\n",
    "# now we are going to clean our data \n",
    "# we will remove stopwords and punctuations and lemmatize each document\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def clean(words):\n",
    "    output=[]\n",
    "    for word in words:\n",
    "        if word.lower() not in stops or word.lower() in not_list:\n",
    "            pos=pos_tag(word)\n",
    "            clean_word=lemmatizer.lemmatize(word,pos=get_simple_pos(pos[0][1]))\n",
    "            output.append(clean_word.lower())\n",
    "    str1=\" \".join(output).encode('utf-8')        \n",
    "    return str1\n",
    "#docs=[ clean(doc) for doc in docs]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "HEdvYfQdWS1m",
    "outputId": "39d0a965-b7b2-4b86-c7a0-82fc12bcd08a"
   },
   "outputs": [],
   "source": [
    "#docs was the cleaned lemmatized text which has been appended into the dataframe for further use\n",
    "#df['CleanedText']=docs\n",
    "#df['CleanedText']=df['CleanedText'].str.decode(\"utf-8\")\n",
    "#def remove_mentions(input_text):\n",
    "        #return re.sub(r'@\\w+', '', input_text)\n",
    "#df.text = df.CleanedText.apply(remove_mentions)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "eywgfErHwShp",
    "outputId": "ff88c88a-69b0-4ca4-8bf4-bb1322e4a212"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0   @user when a father is dysfunctional and is s...      0\n",
       "1  @user @user thanks for #lyft credit i can't us...      0\n",
       "2                                bihday your majesty      0\n",
       "3  #model   i love u take with u all the time in ...      0\n",
       "4             factsguide: society now    #motivation      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking only two columns in the dataframe\n",
    "df=df[['sentence','label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_B9KW6XjAVa_"
   },
   "outputs": [],
   "source": [
    "#taking variables to be used for train test split as X,y\n",
    "X,Y=df['sentence'].values,pd.get_dummies(df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e57a2yp9AVfN",
    "outputId": "d89e109c-b42a-4d18-8630-a8f33bc1c9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 31962 documents\n",
      "15000 words in dictionary\n",
      "Top 5 most common words are: [('user', 17577), ('the', 10165), ('to', 9833), ('a', 6481), ('i', 6170)]\n"
     ]
    }
   ],
   "source": [
    "#using tokenizers to create the tokens having no of words=15000(num_words)\n",
    "tk = Tokenizer(num_words=num_words,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "#Complete data is tokenized to vectors and padding is done using zeros to match its length to the largest text in the dataset.\n",
    "tk.fit_on_texts(X)\n",
    "X = tk.texts_to_sequences(X)\n",
    "X = pad_sequences(X)\n",
    "#print(X[:2])\n",
    "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
    "print('{} words in dictionary'.format(tk.num_words))\n",
    "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GF8DK2-1Y3CA",
    "outputId": "8cea57eb-176e-4da0-af73-f3779fd6b931"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tk,open('transform2.pkl','wb'))\n",
    "#files.download('transform2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CeCPtjnUWIl",
    "outputId": "2c2cf6bd-7027-40c6-9b18-f494cc65eba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: (25569, 39)\n",
      "# Test data samples: (6393, 39)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "print('# Train data samples:', X_train.shape)\n",
    "print('# Test data samples:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hndF1pitCISi",
    "outputId": "5aba6b0b-7180-49c5-c89d-64dd5b68b637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (2557, 39)\n"
     ]
    }
   ],
   "source": [
    "#getting validation data as a part of training data\n",
    "X_train_rest, X_valid, Y_train_rest, Y_valid = train_test_split(X_train,Y_train, test_size=0.1, random_state=37)\n",
    "print('Shape of validation set:',X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EnlWWTk-E4D4"
   },
   "outputs": [],
   "source": [
    "#Function defined to test the models in the test set\n",
    "def test_model(model, epoch_stop):\n",
    "    model.fit(X_test\n",
    "              , Y_test\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=batch_size\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1kMJUhJOnDL",
    "outputId": "7db5949c-b11e-4ce4-843c-b2b7c18d7c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 39, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                6304      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,181,170\n",
      "Trainable params: 2,181,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 #dimension of the word embedding vector for each word in a sequence \n",
    "lstm_out = 196  #no of lstm layers\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(num_words, embed_dim,input_length = X_train.shape[1]))\n",
    "#Adding dropout\n",
    "lstm_model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "#Adding a regularized dense layer\n",
    "lstm_model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n",
    "lstm_model.add(layers.Dropout(0.5))\n",
    "lstm_model.add(Dense(2,activation='softmax'))\n",
    "lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnaNvVukOnIx",
    "outputId": "63acbf00-f1c3-4476-fa39-b2437623e74f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 26s 570ms/step - loss: 0.3581 - accuracy: 0.9216 - val_loss: 0.2437 - val_accuracy: 0.9308\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 24s 528ms/step - loss: 0.1790 - accuracy: 0.9509 - val_loss: 0.1444 - val_accuracy: 0.9578\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 26s 576ms/step - loss: 0.0971 - accuracy: 0.9762 - val_loss: 0.1344 - val_accuracy: 0.9648\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 24s 542ms/step - loss: 0.0700 - accuracy: 0.9844 - val_loss: 0.1498 - val_accuracy: 0.9582\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 26s 577ms/step - loss: 0.0482 - accuracy: 0.9911 - val_loss: 0.1600 - val_accuracy: 0.9550\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 26s 576ms/step - loss: 0.0361 - accuracy: 0.9934 - val_loss: 0.2082 - val_accuracy: 0.9570\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 25s 554ms/step - loss: 0.0284 - accuracy: 0.9957 - val_loss: 0.2027 - val_accuracy: 0.9593\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 27s 590ms/step - loss: 0.0233 - accuracy: 0.9961 - val_loss: 0.2184 - val_accuracy: 0.9566\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 25s 547ms/step - loss: 0.0206 - accuracy: 0.9964 - val_loss: 0.2124 - val_accuracy: 0.9570\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 26s 584ms/step - loss: 0.0180 - accuracy: 0.9970 - val_loss: 0.2561 - val_accuracy: 0.9593\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 26s 588ms/step - loss: 0.0160 - accuracy: 0.9973 - val_loss: 0.2372 - val_accuracy: 0.9593\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 25s 560ms/step - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.2387 - val_accuracy: 0.9554\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 26s 585ms/step - loss: 0.0119 - accuracy: 0.9982 - val_loss: 0.2508 - val_accuracy: 0.9546\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 25s 549ms/step - loss: 0.0088 - accuracy: 0.9989 - val_loss: 0.3088 - val_accuracy: 0.9570\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 27s 589ms/step - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.2678 - val_accuracy: 0.9535\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 25s 545ms/step - loss: 0.0080 - accuracy: 0.9990 - val_loss: 0.3019 - val_accuracy: 0.9535\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 26s 585ms/step - loss: 0.0093 - accuracy: 0.9984 - val_loss: 0.2529 - val_accuracy: 0.9492\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 25s 549ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.2987 - val_accuracy: 0.9542\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 26s 587ms/step - loss: 0.0074 - accuracy: 0.9991 - val_loss: 0.2567 - val_accuracy: 0.9562\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 25s 554ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 0.3017 - val_accuracy: 0.9550\n"
     ]
    }
   ],
   "source": [
    "#model trained on the training data and taking validation data into account to avoid overfitting for 4 epochs \n",
    "history_LSTM=lstm_model.fit(X_train_rest, Y_train_rest, epochs = 20, batch_size=batch_size,validation_data=(X_valid, Y_valid),verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgOA7WOIeQSZ",
    "outputId": "78ea1d40-2847-4860-d5e5-39514a6e2ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0360 - accuracy: 0.9916\n",
      "/n\n",
      "Test accuracy of lstm model: 99.16%\n"
     ]
    }
   ],
   "source": [
    "#prediction by our lstm model on the test dataset\n",
    "lstm_results = test_model(lstm_model, 3)\n",
    "print('/n')\n",
    "print('Test accuracy of lstm model: {0:.2f}%'.format(lstm_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9E-WBbZcYUfz",
    "outputId": "9ad75f61-49f1-45cc-ebc9-6b17b03e24d3"
   },
   "outputs": [],
   "source": [
    "lstm_model.save('lstm_model.h5')\n",
    "#from google.colab import files\n",
    "#files.download('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Airline Sentiment Analysis Using LSTMs with pickle file.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
